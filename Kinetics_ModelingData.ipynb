{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dr Oliviero Andreussi, olivieroandreuss@boisestate.edu\n",
    "\n",
    "Boise State University, Department of Chemistry and Biochemistry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting and Data Analysis for the Kinetics of Methylene Blue RedOx Experiment {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Setup {-}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let us import the main modules that we will need for this lecture. You may see some new modules in the list below, we will add more details in the right sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Notebook Setup { display-mode: \"form\" }\n",
    "# Import the main modules used in this worksheet\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import scipy.optimize\n",
    "# Load the google drive with your files \n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# The following needs to be the path of the folder with all your datafile in .csv format\n",
    "base_path = '/content/drive/MyDrive/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Functions to load the data { display-mode: \"form\" }\n",
    "def load_data_to_file_dict(file_dict):\n",
    "    \"\"\"\n",
    "    Load a uv-vis .csv file. \n",
    "    The format of the file should have two rows of headers and two columns of data (time and absorbance)\n",
    "    The file may have additional information from the instrument saved at the bottom\n",
    "\n",
    "    Input variables:\n",
    "        file_dict : a dictionary with 'path' and 'name' keys corresponding to the file to be loaded\n",
    "    \n",
    "    Action: \n",
    "        Add to file_dict a Pandas DataFrame with two columns: time (in seconds) and absorbance (in input units)   \n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_dict['path']+file_dict['name'],header=1,usecols=(0,1)).apply(pd.to_numeric,errors='coerce').dropna()\n",
    "    if data.keys()[0] == 'Time (sec)' :\n",
    "        data = data.rename(columns={'Time (sec)':'time'})\n",
    "    elif data.keys()[0] == 'Time (min)' :\n",
    "        data['time'] = data['Time (min)'] * 60\n",
    "        data = data.drop('Time (min)',axis=1)\n",
    "    data = data.rename(columns={'Abs':'absorbance'})\n",
    "    file_dict['data'] = data\n",
    "    return\n",
    "\n",
    "def load_data_to_file_list(file_list):\n",
    "    \"\"\"\n",
    "    Given a list of dictionary files, recursively use load_data_to_file_dict to load the data into each of the dictionaries\n",
    "\n",
    "    Input variables:\n",
    "        file_list : a list of dictionary files, each with 'path' and 'name' keys corresponding to the file to be loaded\n",
    "    \n",
    "    Action: \n",
    "        Add to each file_dict a Pandas DataFrame with two columns: time (in seconds) and temperature (in input units)   \n",
    "    \"\"\"\n",
    "    for f in file_list : \n",
    "        if not ('data' in f): load_data_to_file_dict(f)\n",
    "    return\n",
    "\n",
    "\n",
    "def plot_file_dict(file_dict, semilog=False):\n",
    "    \"\"\"\n",
    "    Given a dictionary file of a bomb calorimetry experiment, plot temperature vs. time.\n",
    "\n",
    "    Input variables:\n",
    "        file_dict : a dictionary file with 'path' and 'name' keys corresponding to the file to be loaded\n",
    "    \n",
    "    Action: \n",
    "        Plot absorbance vs. time for the selected file \n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    if not ('data' in file_dict): \n",
    "        load_data_to_file_dict(f)\n",
    "    if semilog :\n",
    "        plt.semilogy(file_dict['data']['time'],file_dict['data']['absorbance'],label=file_dict['name'])\n",
    "    else:\n",
    "        plt.plot(file_dict['data']['time'],file_dict['data']['absorbance'],label=file_dict['name'])\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Absorbance (a.u.)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_by_key(file_list,key='',value=[''],semilog=False):\n",
    "    \"\"\"\n",
    "    Given a list of dictionary files, plot absorbance vs. time for each file into the same plot.\n",
    "    If key/value are specified, only plot the files for which the key has the specified value.\n",
    "\n",
    "    Input variables:\n",
    "        file_list : a list of dictionary files, each with 'path' and 'name' keys corresponding to the file to be loaded\n",
    "        key: a string with the name of the key to shortlist the files\n",
    "        value: the value of the key used to select the shortlist of files\n",
    "    \n",
    "    Action: \n",
    "        Plot absorbance vs. time for the selected files  \n",
    "    \"\"\"\n",
    "    if value == '' or key == '':\n",
    "        file_shortlist = file_list\n",
    "    else :\n",
    "        file_shortlist = [f for f in file_list if f[key] in value ]\n",
    "    fig, ax = plt.subplots()\n",
    "    for f in file_shortlist : \n",
    "        if not ('data' in f): \n",
    "            load_data_to_file_dict(f)\n",
    "        if semilog :\n",
    "            plt.semilogy(f['data']['time'],f['data']['absorbance'],label=f['name'])\n",
    "        else:\n",
    "            plt.plot(f['data']['time'],f['data']['absorbance'],label=f['name'])\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Absorbance (a.u.)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Functions to fit the data { display-mode: \"form\" }\n",
    "def linear(t, a, k, o):\n",
    "    \"\"\" \n",
    "    Function that returns a linear decaying function that saturates to an offset\n",
    "    f(t) = o + a - k * t (for t < a/k) or o (for t > a/k)\n",
    "\n",
    "    input variables\n",
    "    t: input value (units of time)\n",
    "    a: amplitude (units of absorbance or concentration)\n",
    "    k: decay rate (units of 1/time)\n",
    "    o: offset (units of absorbance or concentration)\n",
    "    \"\"\"\n",
    "    filter = t < a/k\n",
    "    return o + filter * (a - k * t)\n",
    "\n",
    "def exponential(t, a, k, o):\n",
    "    \"\"\" \n",
    "    Function that returns an exponentially decaying function plus offset\n",
    "    f(t) = o + a*e^(-k*t)\n",
    "\n",
    "    input variables\n",
    "    t: input value (units of time)\n",
    "    a: amplitude (units of absorbance or concentration)\n",
    "    k: decay rate (units of 1/time)\n",
    "    o: offset (units of absorbance or concentration)\n",
    "    \"\"\"\n",
    "    return a * np.exp(-k*t) + o\n",
    "\n",
    "def inverse(t, a, k, o):\n",
    "    \"\"\" \n",
    "    Function that returns a inverse decaying function plus offset\n",
    "    f(t) = o + 1/(1/a + 2*k*t) \n",
    "\n",
    "    input variables\n",
    "    t: input value (units of time)\n",
    "    a: amplitude (units of absorbance or concentration)\n",
    "    k: decay rate (units of 1/time)\n",
    "    o: offset (units of absorbance or concentration)\n",
    "    \"\"\"\n",
    "    return o + 1/(1/a+2*k*t)\n",
    "\n",
    "# Functions designed for fit the kinetics data\n",
    "def fit_kinetic(file_dict):\n",
    "    \"\"\" \n",
    "    Perform the fit of kinetic decay curves (uv-vis absorption spectra) using non-linear\n",
    "    functions plus offset. \n",
    "\n",
    "    Input parameters: \n",
    "    file_dict: a dictionary with information on the file with the data (path and name) and\n",
    "               adjustable parameters related to the fit:\n",
    "               time_skip : initial transient regime to remove from the fit\n",
    "               order: order of the kinetics to fit (zeroth, first, or second)\n",
    "               MB0_guess, k_guess, offset_guess: starting guess of fitting parameters\n",
    "\n",
    "    Action: \n",
    "           Filter the data by removing the initial time_skip part of the curve\n",
    "           Fit one of three non-linear regimes according to the specified order (linear, exponential, inverse)\n",
    "           Save the optimized values of the parameters and their standard errors in file_dict\n",
    "           Save the fitted curve in the file_dict['data'] DataFrame\n",
    "    \"\"\"   \n",
    "    if not ('data' in file_dict): \n",
    "        load_data_to_file_dict(file_dict)\n",
    "        \n",
    "    filtered_data = file_dict['data'][file_dict['data']['time']>file_dict['time_skip']]\n",
    "    x = filtered_data['time']\n",
    "    y = filtered_data['absorbance']\n",
    "\n",
    "    if file_dict['order'] == 'zeroth' :\n",
    "        funct = linear\n",
    "    elif file_dict['order'] == 'first' :\n",
    "        funct = exponential\n",
    "    elif file_dict['order'] == 'second' :\n",
    "        funct = inverse\n",
    "\n",
    "    p0 = (file_dict['MB0_guess'],file_dict['k_guess'],file_dict['offset_guess'])\n",
    "    params, cv = scipy.optimize.curve_fit(funct,x,y,p0)\n",
    "\n",
    "    file_dict['MB0'] = params[0]\n",
    "    file_dict['MB0_SE'] = np.sqrt(cv[0,0])\n",
    "    file_dict['k'] = params[1]\n",
    "    file_dict['k_SE'] = np.sqrt(cv[1,1])\n",
    "    file_dict['offset'] = params[2] \n",
    "    file_dict['offset_SE'] = np.sqrt(cv[2,2])\n",
    "\n",
    "    file_dict['data']['absorbance_fitted'] = funct(file_dict['data']['time'],params[0],params[1],params[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Systems {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following module needs to be installed on Colab. We won't need it too much for this analysis, but they offer a lot of nice features for chemistry programming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Install and load RDKit { display-mode: \"form\" }\n",
    "!pip install rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "!pip install cirpy\n",
    "import cirpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular we can use them to draw the molecules in our experiments. While for some molecules you can just write their names and RDKit will plot them, for most molecules you will need to provide their SMILES or their CAS numbers. Luckily, CIRpy can usually find SMILES for you, if you type the common name correctly or if you know the CAS number. \n",
    "\n",
    "These are the SMILES for the molecules in your kinetics experiments:\n",
    "* Metylene Blue: '[Cl-].CN(C)c1ccc2nc3ccc(cc3[s+]c2c1)N(C)C'\n",
    "* Leucometylene Blue: 'CN(C)c1ccc2Nc3ccc(cc3Sc2c1)N(C)C'\n",
    "* Ascrobic Acid: 'OCC(O)C1OC(=C(O)C1=O)O'\n",
    "* Ascorbate: 'OC[C@H](O)[C@H]1OC(=O)[C-](O)C1=O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Choose the molecule to draw { display-mode: \"form\" }\n",
    "input = 'ascorbate' # @param {type:\"string\"}\n",
    "input_type = 'name' # @param [\"smiles\", \"name\", \"cas\"] {allow-input: true}\n",
    "if input_type != 'smiles' :\n",
    "    smiles=cirpy.resolve( input, 'smiles')\n",
    "else:\n",
    "    smiles=input\n",
    "img = Draw.MolToImage( Chem.MolFromSmiles(smiles), size=(300, 300) )\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load the Google Drive and access an example of a dataset from a kinetics experiment. You can use the same set that I am using by downloading it from Canvas, [here](). Or you can use your own files. I am assuming the file in question will be located in a `Kinetics_Data/` subfolder in your `Colab Notebook/` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Set Local Path { display-mode: \"form\" }\n",
    "# The following needs to be the path of the folder with all your collected data in .csv format\n",
    "local_path=\"Colab Notebooks/Kinetics_Data/\" # @param {type:\"string\"}\n",
    "path = base_path+local_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to streamline the fitting of the data, we will be storing the file that corresponds to each experiment into a Python dictionary (`dict`), together with all the relevant information of that experiment and the parameters that we need for the fit. You can use the same statement in the following, but make sure to change the file name from 'SetARun1.csv' (the one that I am using) to the one you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = {'path':path, 'name':'SetARun1.csv', 'set':'A', '[MB]': 0.1, '[AA]': 0.1, '[HCl]': 0.1, 'pH': 1, 'order': 'first', 'MB0_guess': 1., 'k_guess': 1., 'offset_guess': 0.1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_to_file_dict(file1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should always check that the loaded data looks ok, say by checking the number of columns and rows and, maybe, plotting the two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1['data'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_file_dict(file1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the same data using a semilog plot (with the y-axis in log scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_file_dict(file1,semilog=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to Fit with Simple Linear Regression {-}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice if we could just use the linear regression algorithm that we have used before to fit the kinetics curves. However, one of our tasks requires to identify the order of the reaction with respect to the concentration of methylene blue (abbreviated in $[MB]$ in the following). Different orders of kinetics correspond to different decay curves. \n",
    "* A reaction that is of zeroth order will have a linear decay in $[MB]$, i.e. $[MB](t)=[MB0]-kt$\n",
    "* A reaction that is of first order will have an exponential decay in $[MB]$, i.e. $[MB](t)=[MB0]e^{-kt}$\n",
    "* A reaction that is of second order will have a decay inversile proportional to $[MB]$, i.e. $[MB](t)=\\frac{1}{1/[MB0]+kt}$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principles, if we make the appropriate change of variables, we can linearize all three of the above equations and try to use linear regression to perform the fit of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroth_order=LinearRegression()\n",
    "x=file1['data']['time'].values.reshape(-1,1)\n",
    "y=file1['data']['absorbance'].values\n",
    "zeroth_order.fit(x,y)\n",
    "plt.plot(x,y)\n",
    "plt.plot(x,zeroth_order.predict(x))\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Absorbance (arbitrary units)')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you try to fit the same data using a first order or a second order model? You will need to change the definition of the dependent variable to make the linear fit appropriate. For the first order you want to fit $log([MB])$ while for the second order model you want to fit $1/[MB]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_order=LinearRegression()\n",
    "x=file1['data']['time'].values.reshape(-1,1)\n",
    "y=np.log(file1['data']['absorbance'].values)\n",
    "first_order.fit(x,y)\n",
    "plt.plot(x,np.exp(y))\n",
    "plt.plot(x,np.exp(first_order.predict(x)))\n",
    "#plt.semilogy(x,np.exp(y))\n",
    "#plt.semilogy(x,np.exp(first_order.predict(x)))\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Absorbance (arbitrary units)')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the models works the best? Is any of the model giving a decent fit of the data? Why or why not? Is the linearization trick always appropriate? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Fit using Scipy {-} "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the possible causes of poor fitting is the fact that our experimental results do not go to zero exactly, but they converge to a baseline, i.e. there is an offset in the data. Unfortunately the offset is tricky to remove and it makes the linearization trick fail. However, we can still try to fit using a non-linear model. The algorithm that optimize an arbitrary model, even those that are non-linear in the parameters, are a bit more tricky to use. They are not able to get the optimal solution directly, but rather they need to optimize the solution one step at the time. The starting point of the algorithm is usually important to ensure convergence and that we get to the best set of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Try alternative non-linear fits { display-mode: \"form\" }\n",
    "order = 'first' # @param ['zeroth', 'first', 'second'] {allow-input: true}\n",
    "semilog = False # @param {type:\"boolean\"}\n",
    "time_skip = 50 # @param {type:\"number\"}\n",
    "\n",
    "MB0 = 0.5 # @param {type:\"number\"}\n",
    "k = 0.1 # @param {type:\"number\"}\n",
    "offset = 0.05 # @param {type:\"number\"}\n",
    "p0 = (MB0, k, offset) # this are the starting values of the parameters of the function\n",
    "\n",
    "if order == 'zeroth' :\n",
    "    funct = linear\n",
    "elif order == 'first' :\n",
    "    funct = exponential\n",
    "elif order == 'second' : \n",
    "    funct = inverse\n",
    "\n",
    "# filter and fit the data\n",
    "filtered_data = file1['data'][file1['data']['time']>time_skip]\n",
    "x = filtered_data['time']\n",
    "y = filtered_data['absorbance']\n",
    "params, cv = scipy.optimize.curve_fit(funct,x,y,p0)\n",
    "\n",
    "# plot the data together with the fit\n",
    "if semilog : \n",
    "    plt.semilogy(file1['data']['time'],file1['data']['absorbance'])\n",
    "    plt.semilogy(file1['data']['time'],funct(file1['data']['time'],params[0],params[1],params[2]))\n",
    "else:\n",
    "    plt.plot(file1['data']['time'],file1['data']['absorbance'])\n",
    "    plt.plot(file1['data']['time'],funct(file1['data']['time'],params[0],params[1],params[2]))\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Absorbance (arbitrary units)')\n",
    "plt.show()\n",
    "\n",
    "# compute correlation coefficient\n",
    "RSS = np.sum((y-funct(x,params[0],params[1],params[2]))**2)\n",
    "TSS = np.sum((y-y.mean())**2)\n",
    "R2 = 1-RSS/TSS\n",
    "\n",
    "# print results\n",
    "print(f\"The optimized values of the parameters are: \\nMB0 = {params[0]:6.4f}, k = {params[1]:6.4f}, offset = {params[2]:6.4f}\")\n",
    "print(f\"The standard errors on these parameters are: \\nSE_MBO = {np.sqrt(cv[0,0]):6.4f}, SE_k = {np.sqrt(cv[1,1]):6.4f}, SE_offset = {np.sqrt(cv[2,2]):6.4f}\")\n",
    "print(f\"The R2 of the fit on the used data is {R2:8.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit returns two objects, which we labeled `params` and `cv`:\n",
    "* the first one contains the value of the optimal parameters, those that give the smallest deviation from the experimental curve\n",
    "* the second object contains the covariance matrix of the parameters: the diagonal of this matrix has the variance in the estimate of the parameter, which you can use as an error estimate for your parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Each of Your Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the analysis above for each one of your curves. Make sure to take note of the important parameters that affect your analysis the most (e.g. `time_skip` and the starting values of the fitting parameters), you will need them in your worksheet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Check the parameters on each of your files { display-mode: \"form\" }\n",
    "name = 'SetBRun3.csv' # @param {type:\"string\"}\n",
    "order = 'first' # @param ['zeroth', 'first', 'second'] {allow-input: true}\n",
    "semilog = False # @param {type:\"boolean\"}\n",
    "time_skip = 40 # @param {type:\"number\"}\n",
    "\n",
    "MB0 = 0.5 # @param {type:\"number\"}\n",
    "k = 0.1 # @param {type:\"number\"}\n",
    "offset = 0.05 # @param {type:\"number\"}\n",
    "p0 = (MB0, k, offset) # this are the starting values of the parameters of the function\n",
    "\n",
    "newfile = {'path':path, 'name':name, 'set':'', '[MB]': 1, '[AA]': 1, '[HCl]': 1, 'pH': 1, 'order': order, 'MB0_guess': MB0, 'k_guess': k, 'offset_guess': offset }\n",
    "load_data_to_file_dict(newfile)\n",
    "\n",
    "if order == 'zeroth' :\n",
    "    funct = linear\n",
    "elif order == 'first' :\n",
    "    funct = exponential\n",
    "elif order == 'second' : \n",
    "    funct = inverse\n",
    "\n",
    "# filter and fit the data\n",
    "filtered_data = newfile['data'][newfile['data']['time']>time_skip]\n",
    "x = filtered_data['time']\n",
    "y = filtered_data['absorbance']\n",
    "params, cv = scipy.optimize.curve_fit(funct,x,y,p0)\n",
    "\n",
    "# plot the data together with the fit\n",
    "if semilog : \n",
    "    plt.semilogy(newfile['data']['time'],newfile['data']['absorbance'])\n",
    "    plt.semilogy(newfile['data']['time'],funct(newfile['data']['time'],params[0],params[1],params[2]))\n",
    "else:\n",
    "    plt.plot(newfile['data']['time'],newfile['data']['absorbance'])\n",
    "    plt.plot(newfile['data']['time'],funct(newfile['data']['time'],params[0],params[1],params[2]))\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Absorbance (arbitrary units)')\n",
    "plt.show()\n",
    "\n",
    "# compute correlation coefficient\n",
    "RSS = np.sum((y-funct(x,params[0],params[1],params[2]))**2)\n",
    "TSS = np.sum((y-y.mean())**2)\n",
    "R2 = 1-RSS/TSS\n",
    "\n",
    "# print results\n",
    "print(f\"The optimized values of the parameters are: \\nMB0 = {params[0]:6.4f}, k = {params[1]:6.4f}, offset = {params[2]:6.4f}\")\n",
    "print(f\"The standard errors on these parameters are: \\nSE_MBO = {np.sqrt(cv[0,0]):6.4f}, SE_k = {np.sqrt(cv[1,1]):6.4f}, SE_offset = {np.sqrt(cv[2,2]):6.4f}\")\n",
    "print(f\"The R2 of the fit on the used data is {R2:8.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
